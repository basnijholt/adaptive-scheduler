{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive-scheduler example\n",
    "\n",
    "[Read the documentation](https://adaptive-scheduler.readthedocs.io/en/latest/#what-is-this) to see what this is all about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: define the simulation\n",
    "\n",
    "Often one wants to sweep a continuous 1D or 2D space for multiple parameters. [Adaptive](http://adaptive.readthedocs.io) is the ideal program to do this. We define a simulation by creating several `adaptive.Learners`. \n",
    "\n",
    "We **need** to define the following variables:\n",
    "* `learners` a list of learners\n",
    "* `fnames` a list of file names, one for each learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import adaptive\n",
    "import adaptive_scheduler\n",
    "\n",
    "\n",
    "def h(x, width=0.01, offset=0):\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    for _ in range(10):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    a = width\n",
    "    return x + a ** 2 / (a ** 2 + (x - offset) ** 2)\n",
    "\n",
    "\n",
    "offsets = [i / 10 - 0.5 for i in range(5)]\n",
    "\n",
    "combos = adaptive.utils.named_product(offset=offsets, width=[0.01, 0.05])\n",
    "\n",
    "learners = []\n",
    "fnames = []\n",
    "\n",
    "for combo in combos:\n",
    "    f = partial(h, **combo)\n",
    "    learner = adaptive.Learner1D(f, bounds=(-1, 1))\n",
    "    fname = adaptive_scheduler.utils.combo2fname(combo, folder=\"data\")\n",
    "    fnames.append(fname)\n",
    "    learners.append(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: run the `learners`\n",
    "\n",
    "After defining the `learners` and `fnames` in an file (above) we can start to run these learners.\n",
    "\n",
    "We split up all learners into seperate jobs, all you need to do is to specify how many cores per job you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptive_scheduler\n",
    "\n",
    "run_manager = adaptive_scheduler.slurm_run(learners, fnames, goal=0.01)\n",
    "run_manager.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or be explicit and use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"example\"\n",
    "scheduler = adaptive_scheduler.scheduler.SLURM(\n",
    "    cores_per_node=2,\n",
    "    nodes=1,\n",
    "    partition=\"hb120rsv2-low\",\n",
    "    executor_type=\"process-pool\",\n",
    "    log_folder=\"logs\",\n",
    ")\n",
    "run_manager = adaptive_scheduler.RunManager(\n",
    "    learners=learners,\n",
    "    fnames=fnames,\n",
    "    scheduler=scheduler,\n",
    "    goal=0.01,\n",
    "    job_name=f\"{name}\",\n",
    "    max_fails_per_job=5,\n",
    "    max_simultaneous_jobs=50,\n",
    "    db_fname=f\"{name}-database.json\",\n",
    "    log_interval=30,\n",
    "    save_interval=30,\n",
    "    save_dataframe=True,\n",
    "    cleanup_first=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the current queue with\n",
    "import pandas as pd\n",
    "\n",
    "queue = run_manager.scheduler.queue(me_only=True)\n",
    "df = pd.DataFrame(queue).transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the logfiles and put it in a `pandas.DataFrame`.\n",
    "# This only returns something when there are log-files to parse!\n",
    "# So after `run_manager.log_interval` has passed.\n",
    "df = run_manager.parse_log_files()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the database\n",
    "df = run_manager.get_database()  # or see `run_manager.database_manager.as_dict()`\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the calculation started and some data has been saved, we can display the learners\n",
    "import adaptive\n",
    "\n",
    "adaptive.notebook_extension()\n",
    "\n",
    "run_manager.load_learners()\n",
    "learner = adaptive.BalancingLearner(learners, cdims=combos)\n",
    "learner.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sequential example\n",
    "Sometimes you cannot formulate your problem with Adaptive, instead you just want to run a function as a sequence of parameters.\n",
    "\n",
    "Surprisingly, this approach with a `SequenceLearner` [is slightly faster than `ipyparallel.Client.map`](https://github.com/python-adaptive/adaptive/pull/193#issuecomment-491062073)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from adaptive import SequenceLearner\n",
    "from adaptive_scheduler.utils import split, combo_to_fname\n",
    "\n",
    "\n",
    "def g(xyz):\n",
    "    x, y, z = xyz\n",
    "    for _ in range(5):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "    return x ** 2 + y ** 2 + z ** 2\n",
    "\n",
    "\n",
    "xs = np.linspace(0, 10, 11)\n",
    "ys = np.linspace(-1, 1, 11)\n",
    "zs = np.linspace(-3, 3, 11)\n",
    "xyzs = [(x, y, z) for x in xs for y in ys for z in zs]\n",
    "\n",
    "# We have only one learner so one fname\n",
    "learners = [SequenceLearner(g, sequence=xyzs)]\n",
    "fnames = [\"data/xyzs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptive_scheduler\n",
    "\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.done()\n",
    "\n",
    "\n",
    "scheduler = adaptive_scheduler.scheduler.DefaultScheduler(\n",
    "    cores=10, executor_type=\"ipyparallel\",\n",
    ")  # PBS or SLURM\n",
    "\n",
    "run_manager2 = adaptive_scheduler.server_support.RunManager(\n",
    "    scheduler, learners, fnames, goal=goal, log_interval=30, save_interval=30,\n",
    ")\n",
    "run_manager2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_manager2.load_learners()\n",
    "learner = learners[0]\n",
    "try:\n",
    "    result = learner.result()\n",
    "    print(result)\n",
    "except:\n",
    "    print(\"`learner.result()` is only available when all values are calculated.\")\n",
    "    partial_data = learner.data\n",
    "    print(partial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended example\n",
    "This example shows how to run split up a list into 100 `SequenceLearner`s and runs it in 100 jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from adaptive import SequenceLearner\n",
    "from adaptive_scheduler.utils import split, combo2fname\n",
    "from adaptive.utils import named_product\n",
    "\n",
    "\n",
    "def g(combo):\n",
    "    x, y, z = combo[\"x\"], combo[\"y\"], combo[\"z\"]\n",
    "\n",
    "    for _ in range(5):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    return x ** 2 + y ** 2 + z ** 2\n",
    "\n",
    "\n",
    "combos = named_product(x=np.linspace(0, 10), y=np.linspace(-1, 1), z=np.linspace(-3, 3))\n",
    "\n",
    "print(f\"Length of combos: {len(combos)}.\")\n",
    "\n",
    "# We could run this as 1 job with N nodes, but we can also split it up in multiple jobs.\n",
    "# This is desireable when you don't want to run a single job with 300 nodes for example.\n",
    "# Note that \n",
    "# `adaptive_scheduler.utils.split_sequence_in_sequence_learners(g, combos, 100, \"data\")`\n",
    "# does the same!\n",
    "\n",
    "njobs = 100\n",
    "split_combos = list(split(combos, njobs))\n",
    "\n",
    "print(\n",
    "    f\"Length of split_combos: {len(split_combos)} and length of split_combos[0]: {len(split_combos[0])}.\"\n",
    ")\n",
    "\n",
    "learners = [SequenceLearner(g, combos_part) for combos_part in split_combos]\n",
    "fnames = [combo2fname(combos_part[0], folder=\"data\") for combos_part in split_combos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the `RunManager` with a lot of arguments to showcase some of the options you can use to customize your run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import adaptive_scheduler\n",
    "from adaptive_scheduler.scheduler import DefaultScheduler, PBS, SLURM\n",
    "\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.done()  # the standard goal for a SequenceLearner\n",
    "\n",
    "\n",
    "extra_scheduler = (\n",
    "    [\"--exclusive\", \"--time=24:00:00\"] if DefaultScheduler is SLURM else []\n",
    ")\n",
    "\n",
    "scheduler = adaptive_scheduler.scheduler.DefaultScheduler(\n",
    "    cores=10,\n",
    "    executor_type=\"ipyparallel\",\n",
    "    extra_scheduler=extra_scheduler,\n",
    "    extra_env_vars=[\"PYTHONPATH='my_dir:$PYTHONPATH'\"],\n",
    "    python_executable=\"~/miniconda3/bin/python\",\n",
    "    log_folder=\"logs\",\n",
    ")  # PBS or SLURM\n",
    "\n",
    "run_manager3 = adaptive_scheduler.server_support.RunManager(\n",
    "    scheduler,\n",
    "    learners,\n",
    "    fnames,\n",
    "    goal=goal,\n",
    "    log_interval=10,\n",
    "    save_interval=30,\n",
    "    runner_kwargs=dict(retries=5, raise_if_retries_exceeded=False),\n",
    "    kill_on_error=\"srun: error:\",  # cancel a job if this is inside a log\n",
    "    job_name=\"example-sequence\",  # this is used to generate unqiue job names\n",
    "    db_fname=\"example-sequence.json\",  # the database keeps track of job_id <-> (learner, is_done)\n",
    "    start_job_manager_kwargs=dict(\n",
    "        max_fails_per_job=10,  # the RunManager is cancelled after njobs * 10 fails\n",
    "        max_simultaneous_jobs=300,  # limit the amount of simultaneous jobs\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_manager3.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_manager3.parse_log_files()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_manager3.load_learners()  # load the data into the learners\n",
    "result = sum(\n",
    "    [l.result() for l in learners], []\n",
    ")  # combine all learner's result into 1 list"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
