{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive-scheduler example\n",
    "\n",
    "[Read the documentation](https://adaptive-scheduler.readthedocs.io/en/latest/#what-is-this) to see what this is all about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: define the simulation\n",
    "\n",
    "Often one wants to sweep a continuous 1D or 2D space for multiple parameters. [Adaptive](http://adaptive.readthedocs.io) is the ideal program to do this. We define a simulation by creating several `adaptive.Learners`. \n",
    "\n",
    "We **need** to define the following variables:\n",
    "* `learners` a list of learners\n",
    "* `fnames` a list of file names, one for each learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile learners_file.py\n",
    "\n",
    "import adaptive\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def h(x, width=0.01, offset=0):\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    for _ in range(10):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    a = width\n",
    "    return x + a ** 2 / (a ** 2 + (x - offset) ** 2)\n",
    "\n",
    "\n",
    "offsets = [i / 10 - 0.5 for i in range(5)]\n",
    "\n",
    "combos = adaptive.utils.named_product(offset=offsets, width=[0.01, 0.05])\n",
    "\n",
    "learners = []\n",
    "fnames = []\n",
    "\n",
    "for combo in combos:\n",
    "    f = partial(h, **combo)\n",
    "    learner = adaptive.Learner1D(f, bounds=(-1, 1))\n",
    "    fnames.append(f\"data/{combo}\")\n",
    "    learners.append(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: run the `learners_file`\n",
    "\n",
    "After defining the `learners` and `fnames` in an file (above) we can start to run these learners.\n",
    "\n",
    "We split up all learners into seperate jobs, all you need to do is to specify how many cores per job you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptive_scheduler\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.npoints > 200\n",
    "\n",
    "scheduler = adaptive_scheduler.scheduler.DefaultScheduler(\n",
    "    cores=10,\n",
    "    executor_type='ipyparallel',\n",
    ")  # PBS or SLURM\n",
    "\n",
    "run_manager = adaptive_scheduler.server_support.RunManager(\n",
    "    scheduler=scheduler,\n",
    "    learners_file=\"learners_file.py\",\n",
    "    goal=goal,\n",
    "    log_interval=30,\n",
    "    save_interval=30,\n",
    ")\n",
    "run_manager.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the current queue with\n",
    "import pandas as pd\n",
    "queue = scheduler.queue()\n",
    "df = pd.DataFrame(queue).transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the logfiles and put it in a `pandas.DataFrame`.\n",
    "# This only returns something when there are log-files to parse!\n",
    "# So after `run_manager.log_interval` has passed.\n",
    "df = run_manager.parse_log_files()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the database\n",
    "df = pd.DataFrame(run_manager.get_database())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the calculation started and some data has been saved, we can display the learners\n",
    "import adaptive\n",
    "from learners_file import learners, fnames, combos\n",
    "from adaptive_scheduler.utils import load_parallel\n",
    "adaptive.notebook_extension()\n",
    "load_parallel(learners, fnames)\n",
    "\n",
    "learner = adaptive.BalancingLearner(learners, cdims=combos)\n",
    "learner.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended example\n",
    "Sometimes you cannot formulate your problem with Adaptive, instead you just want to run a function as a sequence of parameters.\n",
    "\n",
    "Surprisingly, this approach with a `SequenceLearner` [is slightly faster than `ipyparallel.Client.map`](https://github.com/python-adaptive/adaptive/pull/193#issuecomment-491062073)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile learners_file_sequence.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from adaptive_scheduler.sequence_learner import SequenceLearner # https://github.com/python-adaptive/adaptive/pull/193\n",
    "from adaptive_scheduler.utils import split, combo_to_fname\n",
    "from adaptive.utils import named_product\n",
    "\n",
    "\n",
    "def g(combo):\n",
    "    combo = dict(combo)  # the sequence learner passes dicts as tuples\n",
    "    x, y, z = combo['x'], combo['y'], combo['z']\n",
    "\n",
    "    for _ in range(5):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    return x ** 2 + y ** 2 + z ** 2\n",
    "\n",
    "\n",
    "combos = named_product(x=np.linspace(0, 10), y=np.linspace(-1, 1), z=np.linspace(-3, 3))\n",
    "\n",
    "print(f\"Length of combos: {len(combos)}.\")\n",
    "\n",
    "# We could run this as 1 job with N nodes, but we can also split it up in multiple jobs.\n",
    "# This is desireable when you don't want to run a single job with 300 nodes for example.\n",
    "njobs = 100\n",
    "split_combos = list(split(combos, njobs))\n",
    "\n",
    "print(f\"Length of split_combos: {len(split_combos)} and length of split_combos[0]: {len(split_combos[0])}.\")\n",
    "\n",
    "learners, fnames = [], []\n",
    "learners = [SequenceLearner(g, combos_part) for combos_part in split_combos]\n",
    "fnames = [combo_to_fname(combos_part[0], folder=\"data\") for combos_part in split_combos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the `RunManager` with a lot of arguments to showcase some of the options you can use to customize your run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import adaptive_scheduler\n",
    "from adaptive_scheduler.scheduler import DefaultScheduler, PBS, SLURM\n",
    "\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.done()  # the standard goal for a SequenceLearner\n",
    "\n",
    "extra_scheduler = [\"--exclusive\", \"--time=24:00:00\"] if DefaultScheduler is SLURM else []\n",
    "\n",
    "scheduler = adaptive_scheduler.scheduler.DefaultScheduler(\n",
    "    cores=20, \n",
    "    executor_type=\"ipyparallel\",\n",
    "    extra_scheduler=extra_scheduler,\n",
    "    extra_env_vars=[\"PYTHONPATH='my_dir:$PYTHONPATH'\"],\n",
    "    python_executable=\"~/miniconda3/bin/python\",\n",
    "    log_folder=\"logs\",\n",
    ")  # PBS or SLURM\n",
    "\n",
    "run_manager2 = adaptive_scheduler.server_support.RunManager(\n",
    "    scheduler,\n",
    "    goal=goal,\n",
    "    log_interval=10,\n",
    "    save_interval=30,\n",
    "    runner_kwargs=dict(retries=5, raise_if_retries_exceeded=False),\n",
    "    kill_on_error=\"srun: error:\",  # cancel a job if this is inside a log\n",
    "    learners_file=\"learners_file_sequence.py\",  # the file that has `learners` and `fnames`\n",
    "    job_name=\"example-sequence\",  # this is used to generate unqiue job names\n",
    "    db_fname=\"example-sequence.json\",  # the database keeps track of job_id <-> (learner, is_done)\n",
    "    start_job_manager_kwargs=dict(\n",
    "        max_fails_per_job=10,  # the RunManager is cancelled after njobs * 10 fails\n",
    "        max_simultaneous_jobs=300,  # limit the amount of simultaneous jobs\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_manager2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_manager2.parse_log_files()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learners_file_sequence import learners, fnames, combos\n",
    "import adaptive\n",
    "from adaptive_scheduler.utils import load_parallel\n",
    "load_parallel(learners, fnames)\n",
    "result = sum([l.result() for l in learners], [])  # combine all learner's result into 1 list"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
